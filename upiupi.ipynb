{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOId1duDylnAF4xBJ2IPeVY",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from cloudinit.config.cc_disk_setup import device_type\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "# from transformers.models.gpt2.modeling_gpt2 import GPT2Attention"
   ],
   "metadata": {
    "id": "0IP2km1jrBMk",
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:13.941376Z",
     "start_time": "2024-11-21T18:40:11.560147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")",
   "metadata": {
    "id": "QZif_d7Sr_vz",
    "ExecuteTime": {
     "end_time": "2024-11-21T17:57:44.223151Z",
     "start_time": "2024-11-21T17:57:17.082380Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:14.010075Z",
     "start_time": "2024-11-21T18:40:13.979147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from GPT2CustomAttention import CustomGPT2Attention, CustomGPT2FlashAttention\n",
    "from data_loader import preprocess_text, split_in_sequence"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:19.221644Z",
     "start_time": "2024-11-21T18:40:14.120800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = \"cuda\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model_vanilla = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "model_flash = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "config = model_flash.config\n",
    "config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"openai-community/gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:14:05.973545Z",
     "start_time": "2024-11-21T18:14:05.735029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block in model_vanilla.transformer.h:\n",
    "    c_attn_weight = block.attn.c_attn.weight.clone()\n",
    "    c_attn_bias = block.attn.c_attn.bias.clone()\n",
    "    c_proj_weight = block.attn.c_proj.weight.clone()\n",
    "    c_proj_bias = block.attn.c_proj.bias.clone()\n",
    "    \n",
    "    q_weight, k_weight, v_weight = torch.chunk(c_attn_weight, chunks=3, dim=1)\n",
    "    q_bias, k_bias, v_bias = torch.chunk(c_attn_bias, chunks=3, dim=0)\n",
    "\n",
    "    custom_attn = CustomGPT2Attention(nx=config.n_embd, n_ctx=config.n_ctx, config=config)\n",
    "    custom_attn.q_attn.weight.data = q_weight.T.clone()\n",
    "    custom_attn.k_attn.weight.data = k_weight.T.clone()\n",
    "    custom_attn.v_attn.weight.data = v_weight.T.clone()\n",
    " \n",
    "    custom_attn.q_attn.bias.data = q_bias.clone()\n",
    "    custom_attn.k_attn.bias.data = k_bias.clone()\n",
    "    custom_attn.v_attn.bias.data = v_bias.clone()\n",
    "    custom_attn.c_proj.weight.data = c_proj_weight.T.clone()\n",
    "    custom_attn.c_proj.bias.data = c_proj_bias.clone()\n",
    "\n",
    "    block.attn = custom_attn"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:19.435176Z",
     "start_time": "2024-11-21T18:40:19.240337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block_flash in model_flash.transformer.h:\n",
    "    c_attn_weight = block_flash.attn.c_attn.weight.clone()\n",
    "    c_attn_bias = block_flash.attn.c_attn.bias.clone()\n",
    "    c_proj_weight = block_flash.attn.c_proj.weight.clone()\n",
    "    c_proj_bias = block_flash.attn.c_proj.bias.clone()\n",
    "    \n",
    "    q_weight, k_weight, v_weight = torch.chunk(c_attn_weight, chunks=3, dim=1)\n",
    "    q_bias, k_bias, v_bias = torch.chunk(c_attn_bias, chunks=3, dim=0)\n",
    "\n",
    "    custom_attn = CustomGPT2FlashAttention(nx=config.n_embd, n_ctx=config.n_ctx, config=config)\n",
    "    custom_attn.q_attn.weight.data = q_weight.T.clone()\n",
    "    custom_attn.k_attn.weight.data = k_weight.T.clone()\n",
    "    custom_attn.v_attn.weight.data = v_weight.T.clone()\n",
    " \n",
    "    custom_attn.q_attn.bias.data = q_bias.clone()\n",
    "    custom_attn.k_attn.bias.data = k_bias.clone()\n",
    "    custom_attn.v_attn.bias.data = v_bias.clone()\n",
    "    custom_attn.c_proj.weight.data = c_proj_weight.T.clone()\n",
    "    custom_attn.c_proj.bias.data = c_proj_bias.clone()\n",
    "\n",
    "    block_flash.attn = custom_attn"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:19.474854Z",
     "start_time": "2024-11-21T18:40:19.472621Z"
    }
   },
   "cell_type": "code",
   "source": "eos_token = tokenizer.eos_token_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T17:59:41.824621Z",
     "start_time": "2024-11-21T17:57:46.937052Z"
    }
   },
   "cell_type": "code",
   "source": "data = preprocess_text(dataset['train'], min_length=15)",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:06:39.631837Z",
     "start_time": "2024-11-21T17:59:41.850725Z"
    }
   },
   "cell_type": "code",
   "source": "dl_train = split_in_sequence(data, tokenizer, batch_size=4, overlap=64, context_length=256)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:06:41.808102Z",
     "start_time": "2024-11-21T18:06:39.874155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_data, labels = dl_train.dataset.tensors\n",
    "\n",
    "torch.save({\"input_data\": input_data, \"labels\": labels}, \"preprocessed_data_256.pt\")\n",
    "print(\"Данные сохранены в preprocessed_data256.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные сохранены в preprocessed_data256.pt\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:20.901098Z",
     "start_time": "2024-11-21T18:40:19.522938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Загрузка данных с диска\n",
    "saved_data = torch.load(\"preprocessed_data_256.pt\")\n",
    "input_data, labels = saved_data[\"input_data\"], saved_data[\"labels\"]\n",
    "\n",
    "# Создание DataLoader\n",
    "batch_size = 6  # Укажи тот же batch_size, который использовался при сохранении\n",
    "dataset = TensorDataset(input_data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "print(\"Данные успешно загружены и готовы к использованию.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19553/3939054396.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(\"preprocessed_data_256.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены и готовы к использованию.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:23.826036Z",
     "start_time": "2024-11-21T18:40:23.486223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a, b = next(iter(dataloader))\n",
    "a.shape, b.shape, len(dataloader)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 256]), torch.Size([6, 256]), 95329)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:26.699105Z",
     "start_time": "2024-11-21T18:40:26.035770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Adafactor, AdamW\n",
    "\n",
    "optimizer_flash = Adafactor(\n",
    "    model_flash.parameters(),\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None\n",
    ")\n",
    "\n",
    "optimizer_vanilla = Adafactor(\n",
    "    model_vanilla.parameters(),\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:27.341904Z",
     "start_time": "2024-11-21T18:40:27.339592Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()  # Освободить кэш GPU",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:24:59.507258Z",
     "start_time": "2024-11-21T18:15:33.550339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_vanilla = model_vanilla.to(\"cuda\")\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model_vanilla.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        optimizer_vanilla.zero_grad()\n",
    "\n",
    "        outputs = model_vanilla(input_ids=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_vanilla.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 2237/285986 [09:25<19:55:17,  3.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     20\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 21\u001B[0m     \u001B[43moptimizer_vanilla\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Average Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/optimization.py:874\u001B[0m, in \u001B[0;36mAdafactor.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    872\u001B[0m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    873\u001B[0m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRMS\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rms(p_data_fp32)\n\u001B[0;32m--> 874\u001B[0m lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_lr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    876\u001B[0m beta2t \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m math\u001B[38;5;241m.\u001B[39mpow(state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m\"\u001B[39m], group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecay_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    877\u001B[0m update \u001B[38;5;241m=\u001B[39m (grad\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m+\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/optimization.py:797\u001B[0m, in \u001B[0;36mAdafactor._get_lr\u001B[0;34m(param_group, param_state)\u001B[0m\n\u001B[1;32m    795\u001B[0m param_scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n\u001B[1;32m    796\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m param_group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_parameter\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 797\u001B[0m     param_scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mparam_group\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRMS\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m param_scale \u001B[38;5;241m*\u001B[39m rel_step_sz\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:40:35.155206Z",
     "start_time": "2024-11-21T18:40:33.845123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "model_flash = model_flash.to(\"cuda\")\n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = GradScaler()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model_flash.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        optimizer_flash.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model_flash(input_ids=inputs.cuda(), labels=labels.cuda())\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # Backward pass with GradScaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer_flash)\n",
    "        scaler.update()\n",
    "        # total_loss += scaler.unscale_(loss)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/95329 [00:00<23:11:38,  1.14it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 3.70 GiB of which 231.62 MiB is free. Including non-PyTorch memory, this process has 3.05 GiB memory in use. Of the allocated memory 2.61 GiB is allocated by PyTorch, and 331.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Backward pass with GradScaler\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m scaler\u001B[38;5;241m.\u001B[39mstep(optimizer_flash)\n\u001B[1;32m     25\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 3.70 GiB of which 231.62 MiB is free. Including non-PyTorch memory, this process has 3.05 GiB memory in use. Of the allocated memory 2.61 GiB is allocated by PyTorch, and 331.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "Флеш внимание - батч_сайз=2, контекст_лен=512\n",
    "    1000 токенов = 6.35 мин\n",
    "Обычное внимание - батч_сайз=2, контекст_лен=512\n",
    "\n",
    "Флеш внимание + АМП ( халф флоут, 16фп )\n",
    "    1000 токенов 4.40 мин"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T18:38:20.742831Z",
     "start_time": "2024-11-21T18:38:20.740114Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.memory_summary())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   2674 MiB |   3028 MiB |  14126 MiB |  11452 MiB |\n",
      "|       from large pool |   2660 MiB |   3015 MiB |  13963 MiB |  11303 MiB |\n",
      "|       from small pool |     13 MiB |     15 MiB |    162 MiB |    149 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   2674 MiB |   3028 MiB |  14126 MiB |  11452 MiB |\n",
      "|       from large pool |   2660 MiB |   3015 MiB |  13963 MiB |  11303 MiB |\n",
      "|       from small pool |     13 MiB |     15 MiB |    162 MiB |    149 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   2630 MiB |   2983 MiB |  14018 MiB |  11387 MiB |\n",
      "|       from large pool |   2617 MiB |   2969 MiB |  13855 MiB |  11238 MiB |\n",
      "|       from small pool |     13 MiB |     15 MiB |    162 MiB |    149 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3006 MiB |   3154 MiB |   3154 MiB | 151552 KiB |\n",
      "|       from large pool |   2990 MiB |   3138 MiB |   3138 MiB | 151552 KiB |\n",
      "|       from small pool |     16 MiB |     16 MiB |     16 MiB |      0 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 339870 KiB | 339871 KiB |   6673 MiB |   6341 MiB |\n",
      "|       from large pool | 337453 KiB | 337453 KiB |   6514 MiB |   6184 MiB |\n",
      "|       from small pool |   2417 KiB |   3614 KiB |    159 MiB |    156 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     624    |     723    |    2674    |    2050    |\n",
      "|       from large pool |     395    |     422    |    1743    |    1348    |\n",
      "|       from small pool |     229    |     301    |     931    |     702    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     624    |     723    |    2674    |    2050    |\n",
      "|       from large pool |     395    |     422    |    1743    |    1348    |\n",
      "|       from small pool |     229    |     301    |     931    |     702    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     113    |     114    |     114    |       1    |\n",
      "|       from large pool |     105    |     106    |     106    |       1    |\n",
      "|       from small pool |       8    |       8    |       8    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      71    |      85    |    1123    |    1052    |\n",
      "|       from large pool |      31    |      57    |     848    |     817    |\n",
      "|       from small pool |      40    |      40    |     275    |     235    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Changing Attantion Block in GPT2 model."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T14:00:55.807281Z",
     "start_time": "2024-11-20T14:00:55.110555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.use_flash_attn = True\n",
    "model2 = model.to(\"cuda\")\n",
    "generated = model2.generate(\n",
    "    inputs.input_ids.to(\"cuda\"),\n",
    "    max_length=40,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "Hello to open government had that.\n",
      ": 20mm, my_Eo-is B] the our ownance F Systematic's police have '60 times in this is no need a game\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How quantization works"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.863195930Z",
     "start_time": "2024-11-16T12:23:50.814761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Исходные веса в FP32\n",
    "weights = torch.randn((4, 4), dtype=torch.float32)\n",
    "\n",
    "# Преобразование в INT4\n",
    "scale = 15 / weights.abs().max()  # Масштабируем до диапазона INT4\n",
    "weights_int4 = torch.round(weights * scale).clamp(-15, 15).to(torch.int8)\n",
    "\n",
    "# Преобразование в INT6\n",
    "scale = 31 / weights.abs().max()  # Масштабируем до диапазона INT6\n",
    "weights_int6 = torch.round(weights * scale).clamp(-31, 31).to(torch.int8)\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.863419191Z",
     "start_time": "2024-11-16T12:23:55.301512Z"
    }
   },
   "cell_type": "code",
   "source": "weights",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2927,  0.1660, -0.6576,  1.4048],\n",
       "        [ 0.3357, -1.2038, -0.5292,  1.2458],\n",
       "        [-0.0752,  0.0526,  1.4541,  0.2409],\n",
       "        [ 0.2059, -0.1143, -0.9621, -1.1691]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.863722418Z",
     "start_time": "2024-11-16T12:23:59.412326Z"
    }
   },
   "cell_type": "code",
   "source": "weights_int4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 13,   2,  -7,  14],\n",
       "        [  3, -12,  -5,  13],\n",
       "        [ -1,   1,  15,   2],\n",
       "        [  2,  -1, -10, -12]], dtype=torch.int8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.864003343Z",
     "start_time": "2024-11-16T12:24:06.728085Z"
    }
   },
   "cell_type": "code",
   "source": "weights_int6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28,   4, -14,  30],\n",
       "        [  7, -26, -11,  27],\n",
       "        [ -2,   1,  31,   5],\n",
       "        [  4,  -2, -21, -25]], dtype=torch.int8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.864261991Z",
     "start_time": "2024-11-16T12:24:40.261146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_restored = weights_int6 / scale\n",
    "weights_restored"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3133,  0.1876, -0.6567,  1.4072],\n",
       "        [ 0.3283, -1.2195, -0.5160,  1.2664],\n",
       "        [-0.0938,  0.0469,  1.4541,  0.2345],\n",
       "        [ 0.1876, -0.0938, -0.9850, -1.1726]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.864519341Z",
     "start_time": "2024-11-16T12:24:57.937523Z"
    }
   },
   "cell_type": "code",
   "source": "weights_restored - weights",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0207,  0.0216,  0.0009,  0.0023],\n",
       "        [-0.0073, -0.0158,  0.0132,  0.0207],\n",
       "        [-0.0186, -0.0057,  0.0000, -0.0063],\n",
       "        [-0.0182,  0.0205, -0.0229, -0.0035]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T20:20:00.407269Z",
     "start_time": "2024-11-20T20:19:58.229705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device_type = \"cuda\"\n",
    "torch.amp.autocast_mode.is_autocast_available(device_type)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T20:21:59.931359Z",
     "start_time": "2024-11-20T20:21:59.672474Z"
    }
   },
   "cell_type": "code",
   "source": "scaler = torch.GradScaler()",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T11:35:01.673964Z",
     "start_time": "2024-11-21T11:34:58.171220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# Загрузи токенизатор и модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Тест: генерация текста\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '~/.cache/huggingface/hub/models--openai-community--gpt2'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 403\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 106\u001B[0m     \u001B[43mvalidate_repo_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repo_id\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must be in the form \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnamespace/repo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    156\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Use `repo_type` argument if needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    157\u001B[0m     )\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n",
      "\u001B[0;31mHFValidationError\u001B[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '~/.cache/huggingface/hub/models--openai-community--gpt2'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m tokenizer_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m~/.cache/huggingface/hub/models--openai-community--gpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Загрузи токенизатор и модель\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_path, local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Тест: генерация текста\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:857\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[39;00m\n\u001B[0;32m--> 857\u001B[0m tokenizer_config \u001B[38;5;241m=\u001B[39m \u001B[43mget_tokenizer_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m tokenizer_config:\n\u001B[1;32m    859\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:689\u001B[0m, in \u001B[0;36mget_tokenizer_config\u001B[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001B[0m\n\u001B[1;32m    686\u001B[0m     token \u001B[38;5;241m=\u001B[39m use_auth_token\n\u001B[1;32m    688\u001B[0m commit_hash \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 689\u001B[0m resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43mTOKENIZER_CONFIG_FILE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_gated_repo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/utils/hub.py:469\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere was a specific connection error when trying to load \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HFValidationError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 469\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    470\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncorrect path_or_model_id: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    471\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n",
      "\u001B[0;31mOSError\u001B[0m: Incorrect path_or_model_id: '~/.cache/huggingface/hub/models--openai-community--gpt2'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
