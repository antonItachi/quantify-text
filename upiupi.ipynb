{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOId1duDylnAF4xBJ2IPeVY",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "# from transformers.models.gpt2.modeling_gpt2 import GPT2Attention"
   ],
   "metadata": {
    "id": "0IP2km1jrBMk",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:35.407267Z",
     "start_time": "2024-11-19T14:09:33.293123Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")['test']",
   "metadata": {
    "id": "QZif_d7Sr_vz",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:40.511324Z",
     "start_time": "2024-11-19T14:09:35.419003Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:40.610446Z",
     "start_time": "2024-11-19T14:09:40.602038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "class CustomGPT2Attention(torch.nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False, layer_idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        n_state = nx  # hidden_dim (n_embd)\n",
    "        assert n_state % config.n_head == 0, \"n_state must be divisible by n_head\"\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = n_state // config.n_head  # Размер одного \"head\"\n",
    "        self.scale = scale\n",
    "        self.layer_idx = layer_idx\n",
    "        self.config = config\n",
    "\n",
    "        # Линейные слои для Q, K, V\n",
    "        self.q_attn = torch.nn.Linear(nx, n_state, bias=True)\n",
    "        self.k_attn = torch.nn.Linear(nx, n_state, bias=True)\n",
    "        self.v_attn = torch.nn.Linear(nx, n_state, bias=True)\n",
    "\n",
    "        # Проекция после внимания\n",
    "        self.c_proj = torch.nn.Linear(n_state, nx, bias=True)\n",
    "\n",
    "        # Dropout\n",
    "        self.attn_dropout = torch.nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = torch.nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        # Маски\n",
    "        self.register_buffer(\n",
    "            \"bias\", torch.tril(torch.ones((n_ctx, n_ctx), dtype=torch.uint8)).view(1, 1, n_ctx, n_ctx)\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4))\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split heads without separate logic for key or query to ensure compatibility.\n",
    "        \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, self.head_dim)\n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0, 2, 1, 3)  # [batch, num_heads, seq_length, head_dim]\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (self.n_head * self.head_dim,)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def _attn(self, q, k, v, attention_mask=None, head_mask=None, output_attentions=False):\n",
    "        w = torch.matmul(q, k)\n",
    "\n",
    "        # Scaling\n",
    "        if self.scale:\n",
    "            w = w / (self.head_dim ** 0.5)\n",
    "        if getattr(self.config, \"scale_attn_by_inverse_layer_idx\", False):\n",
    "            w = w / float(self.layer_idx + 1)\n",
    "\n",
    "        # Apply causal mask\n",
    "        mask = self.bias[:, :, : w.size(-2), : w.size(-1)]\n",
    "        w = torch.where(mask.bool(), w, self.masked_bias.to(w.dtype))\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            w = w + attention_mask\n",
    "\n",
    "        w = nn.functional.softmax(w, dim=-1)\n",
    "        w = self.attn_dropout(w)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            w = w * head_mask\n",
    "\n",
    "        outputs = (torch.matmul(w, v),)\n",
    "        if output_attentions:\n",
    "            outputs += (w,)\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False):\n",
    "        query = self.q_attn(hidden_states)\n",
    "        key = self.k_attn(hidden_states)\n",
    "        value = self.v_attn(hidden_states)\n",
    "    \n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "    \n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "    \n",
    "        if use_cache:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "    \n",
    "        attn_outputs = self._attn(query, key.transpose(-1, -2), value, attention_mask, head_mask, output_attentions)\n",
    "        a = attn_outputs[0]\n",
    "    \n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a)\n",
    "    \n",
    "        outputs = (a, present) + attn_outputs[1:]\n",
    "        return outputs"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:42.069979Z",
     "start_time": "2024-11-19T14:09:40.655793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = \"cuda\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "config = model.config\n",
    "config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"openai-community/gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:03:34.993343Z",
     "start_time": "2024-11-19T14:03:34.795445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block in model.transformer.h:\n",
    "    c_attn_weight = block.attn.c_attn.weight.clone()\n",
    "    c_attn_bias = block.attn.c_attn.bias.clone()\n",
    "    c_proj_weight = block.attn.c_proj.weight.clone()\n",
    "    c_proj_bias = block.attn.c_proj.bias.clone()\n",
    "    \n",
    "    \n",
    "    q_weight, k_weight, v_weight = torch.chunk(c_attn_weight, chunks=3, dim=1)\n",
    "    q_bias, k_bias, v_bias = torch.chunk(c_attn_bias, chunks=3, dim=0)\n",
    "\n",
    "    custom_attn = CustomGPT2Attention(nx=config.n_embd, n_ctx=config.n_ctx, config=config)\n",
    "    custom_attn.q_attn.weight.data = q_weight.T.clone()\n",
    "    custom_attn.k_attn.weight.data = k_weight.T.clone()\n",
    "    custom_attn.v_attn.weight.data = v_weight.T.clone()\n",
    " \n",
    "    \n",
    "    custom_attn.q_attn.bias.data = q_bias.clone()\n",
    "    custom_attn.k_attn.bias.data = k_bias.clone()\n",
    "    custom_attn.v_attn.bias.data = v_bias.clone()\n",
    "    custom_attn.c_proj.weight.data = c_proj_weight.T.clone()\n",
    "    custom_attn.c_proj.bias.data = c_proj_bias.clone()\n",
    "\n",
    "    block.attn = custom_attn"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:42.155326Z",
     "start_time": "2024-11-19T14:09:42.152243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = model.config.n_positions\n",
    "eos_token = tokenizer.eos_token_id\n",
    "context_length, eos_token"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 50256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:42.242041Z",
     "start_time": "2024-11-19T14:09:42.239175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:42.367076Z",
     "start_time": "2024-11-19T14:09:42.354219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_data = [text for text in dataset['text'] if len(text) > 3]\n",
    "filtered_data = [line.replace(\"\\n\", \" \").strip() for line in filtered_data]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:44.129516Z",
     "start_time": "2024-11-19T14:09:42.946211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 512\n",
    "current_sequence = torch.tensor([], dtype=torch.long)\n",
    "input_batch = []\n",
    "\n",
    "for element in filtered_data:\n",
    "    outputs = tokenizer(\n",
    "        element,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = outputs[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    current_sequence = torch.cat([current_sequence, input_ids])\n",
    "\n",
    "    while len(current_sequence) >= context_length:\n",
    "        input_batch.append(current_sequence[:context_length])\n",
    "        current_sequence = current_sequence[context_length:]\n",
    "\n",
    "if len(current_sequence) > 0:\n",
    "    input_batch.append(\n",
    "        torch.cat([current_sequence, torch.tensor([eos_token])])\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:45.209034Z",
     "start_time": "2024-11-19T14:09:45.206642Z"
    }
   },
   "cell_type": "code",
   "source": "input_batch = input_batch[:-1] # last token is less than 1024",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:07:07.181334Z",
     "start_time": "2024-11-19T14:07:07.178698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(input_batch))\n",
    "print(len(input_batch))\n",
    "print(type(input_batch[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "542\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:47.582348Z",
     "start_time": "2024-11-19T14:09:47.578713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input_data = torch.stack(input_batch)\n",
    "labels = input_data.clone()\n",
    "\n",
    "dataset = TensorDataset(input_data, labels)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:49.327548Z",
     "start_time": "2024-11-19T14:09:49.063558Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:50.162667Z",
     "start_time": "2024-11-19T14:09:49.602392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Adafactor\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(),\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:52.410064Z",
     "start_time": "2024-11-19T14:09:52.406200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = model.to(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:09:54.645478Z",
     "start_time": "2024-11-19T14:09:54.098846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/136 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 3.70 GiB of which 120.62 MiB is free. Including non-PyTorch memory, this process has 3.05 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 52.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 14\u001B[0m\n\u001B[1;32m     10\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m), labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 14\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     16\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1300\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1298\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(lm_logits\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   1299\u001B[0m \u001B[38;5;66;03m# Shift so that tokens < n predict n\u001B[39;00m\n\u001B[0;32m-> 1300\u001B[0m shift_logits \u001B[38;5;241m=\u001B[39m \u001B[43mlm_logits\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1301\u001B[0m shift_labels \u001B[38;5;241m=\u001B[39m labels[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m   1302\u001B[0m \u001B[38;5;66;03m# Flatten the tokens\u001B[39;00m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 3.70 GiB of which 120.62 MiB is free. Including non-PyTorch memory, this process has 3.05 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 52.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Changing Attantion Block in GPT2 model."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:38:31.774236Z",
     "start_time": "2024-11-19T13:38:31.337825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2 = model.to(\"cuda\")\n",
    "generated = model2.generate(\n",
    "    inputs.input_ids.to(\"cuda\"),\n",
    "    max_length=40,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this world because of   in that, or not \". I and\n",
      " for with- it a 2_s is from there ( an so all to have the on by also's\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How quantization works"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:23:50.848421Z",
     "start_time": "2024-11-16T12:23:50.814761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Исходные веса в FP32\n",
    "weights = torch.randn((4, 4), dtype=torch.float32)\n",
    "\n",
    "# Преобразование в INT4\n",
    "scale = 15 / weights.abs().max()  # Масштабируем до диапазона INT4\n",
    "weights_int4 = torch.round(weights * scale).clamp(-15, 15).to(torch.int8)\n",
    "\n",
    "# Преобразование в INT6\n",
    "scale = 31 / weights.abs().max()  # Масштабируем до диапазона INT6\n",
    "weights_int6 = torch.round(weights * scale).clamp(-31, 31).to(torch.int8)\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:23:55.306499Z",
     "start_time": "2024-11-16T12:23:55.301512Z"
    }
   },
   "cell_type": "code",
   "source": "weights",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2927,  0.1660, -0.6576,  1.4048],\n",
       "        [ 0.3357, -1.2038, -0.5292,  1.2458],\n",
       "        [-0.0752,  0.0526,  1.4541,  0.2409],\n",
       "        [ 0.2059, -0.1143, -0.9621, -1.1691]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:23:59.419307Z",
     "start_time": "2024-11-16T12:23:59.412326Z"
    }
   },
   "cell_type": "code",
   "source": "weights_int4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 13,   2,  -7,  14],\n",
       "        [  3, -12,  -5,  13],\n",
       "        [ -1,   1,  15,   2],\n",
       "        [  2,  -1, -10, -12]], dtype=torch.int8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:24:06.744578Z",
     "start_time": "2024-11-16T12:24:06.728085Z"
    }
   },
   "cell_type": "code",
   "source": "weights_int6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28,   4, -14,  30],\n",
       "        [  7, -26, -11,  27],\n",
       "        [ -2,   1,  31,   5],\n",
       "        [  4,  -2, -21, -25]], dtype=torch.int8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:24:40.275108Z",
     "start_time": "2024-11-16T12:24:40.261146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_restored = weights_int6 / scale\n",
    "weights_restored"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3133,  0.1876, -0.6567,  1.4072],\n",
       "        [ 0.3283, -1.2195, -0.5160,  1.2664],\n",
       "        [-0.0938,  0.0469,  1.4541,  0.2345],\n",
       "        [ 0.1876, -0.0938, -0.9850, -1.1726]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:24:57.955475Z",
     "start_time": "2024-11-16T12:24:57.937523Z"
    }
   },
   "cell_type": "code",
   "source": "weights_restored - weights",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0207,  0.0216,  0.0009,  0.0023],\n",
       "        [-0.0073, -0.0158,  0.0132,  0.0207],\n",
       "        [-0.0186, -0.0057,  0.0000, -0.0063],\n",
       "        [-0.0182,  0.0205, -0.0229, -0.0035]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
