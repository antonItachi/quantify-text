{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOId1duDylnAF4xBJ2IPeVY",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from cloudinit.config.cc_disk_setup import device_type\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "# from transformers.models.gpt2.modeling_gpt2 import GPT2Attention"
   ],
   "metadata": {
    "id": "0IP2km1jrBMk",
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:35.217924Z",
     "start_time": "2024-11-20T15:21:34.439764Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")['test']",
   "metadata": {
    "id": "QZif_d7Sr_vz",
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:40.150875Z",
     "start_time": "2024-11-20T15:21:35.224071Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:20:44.883349Z",
     "start_time": "2024-11-20T15:20:42.898449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from GPT2CustomAttention import CustomGPT2Attention, CustomGPT2FlashAttention\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:20:47.482274Z",
     "start_time": "2024-11-20T15:20:44.897331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = \"cuda\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model_vanilla = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "model_flash = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "config = model_vanilla.config\n",
    "config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"openai-community/gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:20:48.519025Z",
     "start_time": "2024-11-20T15:20:48.222303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block in model_vanilla.transformer.h:\n",
    "    c_attn_weight = block.attn.c_attn.weight.clone()\n",
    "    c_attn_bias = block.attn.c_attn.bias.clone()\n",
    "    c_proj_weight = block.attn.c_proj.weight.clone()\n",
    "    c_proj_bias = block.attn.c_proj.bias.clone()\n",
    "    \n",
    "    q_weight, k_weight, v_weight = torch.chunk(c_attn_weight, chunks=3, dim=1)\n",
    "    q_bias, k_bias, v_bias = torch.chunk(c_attn_bias, chunks=3, dim=0)\n",
    "\n",
    "    custom_attn = CustomGPT2Attention(nx=config.n_embd, n_ctx=config.n_ctx, config=config)\n",
    "    custom_attn.q_attn.weight.data = q_weight.T.clone()\n",
    "    custom_attn.k_attn.weight.data = k_weight.T.clone()\n",
    "    custom_attn.v_attn.weight.data = v_weight.T.clone()\n",
    " \n",
    "    custom_attn.q_attn.bias.data = q_bias.clone()\n",
    "    custom_attn.k_attn.bias.data = k_bias.clone()\n",
    "    custom_attn.v_attn.bias.data = v_bias.clone()\n",
    "    custom_attn.c_proj.weight.data = c_proj_weight.T.clone()\n",
    "    custom_attn.c_proj.bias.data = c_proj_bias.clone()\n",
    "\n",
    "    block.attn = custom_attn"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:20:49.401864Z",
     "start_time": "2024-11-20T15:20:49.090051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block_flash in model_flash.transformer.h:\n",
    "    c_attn_weight = block_flash.attn.c_attn.weight.clone()\n",
    "    c_attn_bias = block_flash.attn.c_attn.bias.clone()\n",
    "    c_proj_weight = block_flash.attn.c_proj.weight.clone()\n",
    "    c_proj_bias = block_flash.attn.c_proj.bias.clone()\n",
    "    \n",
    "    q_weight, k_weight, v_weight = torch.chunk(c_attn_weight, chunks=3, dim=1)\n",
    "    q_bias, k_bias, v_bias = torch.chunk(c_attn_bias, chunks=3, dim=0)\n",
    "\n",
    "    custom_attn = CustomGPT2FlashAttention(nx=config.n_embd, n_ctx=config.n_ctx, config=config)\n",
    "    custom_attn.q_attn.weight.data = q_weight.T.clone()\n",
    "    custom_attn.k_attn.weight.data = k_weight.T.clone()\n",
    "    custom_attn.v_attn.weight.data = v_weight.T.clone()\n",
    " \n",
    "    custom_attn.q_attn.bias.data = q_bias.clone()\n",
    "    custom_attn.k_attn.bias.data = k_bias.clone()\n",
    "    custom_attn.v_attn.bias.data = v_bias.clone()\n",
    "    custom_attn.c_proj.weight.data = c_proj_weight.T.clone()\n",
    "    custom_attn.c_proj.bias.data = c_proj_bias.clone()\n",
    "\n",
    "    block_flash.attn = custom_attn"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:20:51.849096Z",
     "start_time": "2024-11-20T15:20:51.845557Z"
    }
   },
   "cell_type": "code",
   "source": "eos_token = tokenizer.eos_token_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:20:53.112787Z",
     "start_time": "2024-11-20T15:20:53.109043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = \"Hello my lovely family and my brother.\"\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:09.753647Z",
     "start_time": "2024-11-20T15:21:08.928575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "model2 = model_vanilla.to(\"cuda\")\n",
    "generated = model2.generate(\n",
    "    inputs.input_ids.to(\"cuda\"),\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my lovely family and my brother. ( the more,- or -\n",
      "? a\" to I are by in for) we of one: is ''s that when A/ 1 \"â€”\n",
      "\n",
      " not about as on with this' it but\n",
      "CPU times: user 823 ms, sys: 1.37 ms, total: 824 ms\n",
      "Wall time: 821 ms\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:12.430834Z",
     "start_time": "2024-11-20T15:21:11.582693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "model2 = model_flash.to(\"cuda\")\n",
    "generated = model2.generate(\n",
    "    inputs.input_ids.to(\"cuda\"),\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my lovely family and my brother. of, in B-G Gout I/a\n",
      " 'It is the celled people). The Ever a many more than an \"The three or his first to be at ranna after your team's\n",
      "CPU times: user 833 ms, sys: 1.83 ms, total: 835 ms\n",
      "Wall time: 841 ms\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.860309149Z",
     "start_time": "2024-11-20T12:32:06.031605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:41.998760Z",
     "start_time": "2024-11-20T15:21:41.986231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_data = [text for text in dataset['text'] if len(text) > 3]\n",
    "filtered_data = [line.replace(\"\\n\", \" \").strip() for line in filtered_data]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:45.050542Z",
     "start_time": "2024-11-20T15:21:43.504306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 512\n",
    "current_sequence = torch.tensor([], dtype=torch.long)\n",
    "input_batch = []\n",
    "\n",
    "for element in filtered_data:\n",
    "    outputs = tokenizer(\n",
    "        element,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = outputs[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    current_sequence = torch.cat([current_sequence, input_ids])\n",
    "\n",
    "    while len(current_sequence) >= context_length:\n",
    "        input_batch.append(current_sequence[:context_length])\n",
    "        current_sequence = current_sequence[context_length:]\n",
    "\n",
    "if len(current_sequence) > 0:\n",
    "    input_batch.append(\n",
    "        torch.cat([current_sequence, torch.tensor([eos_token])])\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:46.068387Z",
     "start_time": "2024-11-20T15:21:46.065159Z"
    }
   },
   "cell_type": "code",
   "source": "input_batch = input_batch[:-1] # last token is less than 1024",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:47.106338Z",
     "start_time": "2024-11-20T15:21:47.102566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(input_batch))\n",
    "print(len(input_batch))\n",
    "print(type(input_batch[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "542\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:49.453358Z",
     "start_time": "2024-11-20T15:21:49.449030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input_data = torch.stack(input_batch)\n",
    "labels = input_data.clone()\n",
    "\n",
    "dataset = TensorDataset(input_data, labels)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:21:51.048675Z",
     "start_time": "2024-11-20T15:21:51.045329Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:22:15.818714Z",
     "start_time": "2024-11-20T15:22:15.803492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Adafactor\n",
    "\n",
    "optimizer_vanilla = Adafactor(\n",
    "    model_vanilla.parameters(),\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None\n",
    ")\n",
    "\n",
    "optimizer_flash = Adafactor(\n",
    "    model_flash.parameters(),\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True,\n",
    "    lr=None\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:22:46.139931Z",
     "start_time": "2024-11-20T15:22:46.131837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_vanilla = model_vanilla.to(\"cuda\")\n",
    "model_flash = model_flash.to(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T15:23:33.959627Z",
     "start_time": "2024-11-20T15:23:33.704653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model_flash.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "        optimizer_flash.zero_grad()\n",
    "\n",
    "        outputs = model_flash(input_ids=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_flash.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/136 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.70 GiB of which 42.19 MiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 100.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 14\u001B[0m\n\u001B[1;32m     10\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m), labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m optimizer_flash\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 14\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_flash\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     16\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1271\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1263\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1264\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[1;32m   1266\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[1;32m   1267\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[1;32m   1268\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1269\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1271\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1276\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1278\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1279\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1280\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1281\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1282\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1286\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1288\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1132\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1120\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m   1121\u001B[0m         block\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m   1122\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1129\u001B[0m         output_attentions,\n\u001B[1;32m   1130\u001B[0m     )\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1132\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1133\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1139\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1140\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1141\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1143\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:615\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    613\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    614\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[0;32m--> 615\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    623\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[1;32m    624\u001B[0m outputs \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/ai/quantify-text/GPT2CustomAttention.py:101\u001B[0m, in \u001B[0;36mCustomGPT2FlashAttention.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m     98\u001B[0m num_heads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_head\n\u001B[1;32m     99\u001B[0m head_dim \u001B[38;5;241m=\u001B[39m hidden_dim \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m num_heads\n\u001B[0;32m--> 101\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m key \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39mreshape(batch, tgt_len, num_heads, head_dim)\n\u001B[1;32m    103\u001B[0m value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mreshape(batch, tgt_len, num_heads, head_dim)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.70 GiB of which 42.19 MiB is free. Including non-PyTorch memory, this process has 3.09 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 100.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:48:23.658264Z",
     "start_time": "2024-11-20T13:48:23.653823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = \"Hello\"\n",
    "\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Changing Attantion Block in GPT2 model."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T14:00:55.807281Z",
     "start_time": "2024-11-20T14:00:55.110555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.use_flash_attn = True\n",
    "model2 = model.to(\"cuda\")\n",
    "generated = model2.generate(\n",
    "    inputs.input_ids.to(\"cuda\"),\n",
    "    max_length=40,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "sobaka torch.Size([1, 1, 12, 64]) torch.Size([1, 12, 64])\n",
      "Hello to open government had that.\n",
      ": 20mm, my_Eo-is B] the our ownance F Systematic's police have '60 times in this is no need a game\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How quantization works"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.863195930Z",
     "start_time": "2024-11-16T12:23:50.814761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð²ÐµÑÐ° Ð² FP32\n",
    "weights = torch.randn((4, 4), dtype=torch.float32)\n",
    "\n",
    "# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² INT4\n",
    "scale = 15 / weights.abs().max()  # ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð¾ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð° INT4\n",
    "weights_int4 = torch.round(weights * scale).clamp(-15, 15).to(torch.int8)\n",
    "\n",
    "# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² INT6\n",
    "scale = 31 / weights.abs().max()  # ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð¾ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð° INT6\n",
    "weights_int6 = torch.round(weights * scale).clamp(-31, 31).to(torch.int8)\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.863419191Z",
     "start_time": "2024-11-16T12:23:55.301512Z"
    }
   },
   "cell_type": "code",
   "source": "weights",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2927,  0.1660, -0.6576,  1.4048],\n",
       "        [ 0.3357, -1.2038, -0.5292,  1.2458],\n",
       "        [-0.0752,  0.0526,  1.4541,  0.2409],\n",
       "        [ 0.2059, -0.1143, -0.9621, -1.1691]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.863722418Z",
     "start_time": "2024-11-16T12:23:59.412326Z"
    }
   },
   "cell_type": "code",
   "source": "weights_int4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 13,   2,  -7,  14],\n",
       "        [  3, -12,  -5,  13],\n",
       "        [ -1,   1,  15,   2],\n",
       "        [  2,  -1, -10, -12]], dtype=torch.int8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.864003343Z",
     "start_time": "2024-11-16T12:24:06.728085Z"
    }
   },
   "cell_type": "code",
   "source": "weights_int6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28,   4, -14,  30],\n",
       "        [  7, -26, -11,  27],\n",
       "        [ -2,   1,  31,   5],\n",
       "        [  4,  -2, -21, -25]], dtype=torch.int8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.864261991Z",
     "start_time": "2024-11-16T12:24:40.261146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_restored = weights_int6 / scale\n",
    "weights_restored"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3133,  0.1876, -0.6567,  1.4072],\n",
       "        [ 0.3283, -1.2195, -0.5160,  1.2664],\n",
       "        [-0.0938,  0.0469,  1.4541,  0.2345],\n",
       "        [ 0.1876, -0.0938, -0.9850, -1.1726]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T13:05:22.864519341Z",
     "start_time": "2024-11-16T12:24:57.937523Z"
    }
   },
   "cell_type": "code",
   "source": "weights_restored - weights",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0207,  0.0216,  0.0009,  0.0023],\n",
       "        [-0.0073, -0.0158,  0.0132,  0.0207],\n",
       "        [-0.0186, -0.0057,  0.0000, -0.0063],\n",
       "        [-0.0182,  0.0205, -0.0229, -0.0035]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T20:20:00.407269Z",
     "start_time": "2024-11-20T20:19:58.229705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device_type = \"cuda\"\n",
    "torch.amp.autocast_mode.is_autocast_available(device_type)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T20:21:59.931359Z",
     "start_time": "2024-11-20T20:21:59.672474Z"
    }
   },
   "cell_type": "code",
   "source": "scaler = torch.GradScaler()",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
